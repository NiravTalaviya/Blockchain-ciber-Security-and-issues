
@inproceedings{ledig_photo-realistic_2017,
	location = {Honolulu, {HI}},
	title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099502/},
	doi = {10.1109/CVPR.2017.19},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the ﬁner texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the ﬁdelity expected at the higher resolution. In this paper, we present {SRGAN}, a generative adversarial network ({GAN}) for image superresolution ({SR}). To our knowledge, it is the ﬁrst framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score ({MOS}) test shows hugely signiﬁcant gains in perceptual quality using {SRGAN}. The {MOS} scores obtained with {SRGAN} are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {105--114},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	urldate = {2019-07-22},
	date = {2017-07},
	langid = {english},
	file = {Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Ledig et al/Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:application/pdf}
}

@article{al-falluji_single_nodate,
	title = {Single Image Super Resolution Algorithms: A Survey and Evaluation},
	volume = {6},
	abstract = {Image processing sub branch that specifically deals with the improvement, of images and videos, resolution without compromising the detail and visual effect but rather enhances the two, is known as Super Resolution. Multiple (multiple input images and one output image) or single (one input and one output) low resolution images are converted to high resolution. Single image super resolution algorithms are more practical since multiple images are not always available. The paper presents a survey of recent single image super resolution methods that are based on the use of external database to predict the values of missing pixels in high resolution image.},
	pages = {8},
	number = {9},
	author = {Al-falluji, Ruaa Adeeb Abdulmunem and Youssif, Aliaa Abdel-Halim and Guirguis, Shawkat K},
	langid = {english},
	file = {Al-falluji et al_Single Image Super Resolution Algorithms.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Al-falluji et al/Al-falluji et al_Single Image Super Resolution Algorithms.pdf:application/pdf}
}

@article{dong_image_2014,
	title = {Image Super-Resolution Using Deep Convolutional Networks},
	url = {http://arxiv.org/abs/1501.00092},
	abstract = {We propose a deep learning method for single image super-resolution ({SR}). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network ({CNN}) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based {SR} methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep {CNN} has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve tradeoffs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
	journaltitle = {{arXiv}:1501.00092 [cs]},
	author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
	urldate = {2019-07-22},
	date = {2014-12-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1501.00092},
	file = {Dong et al_2014_Image Super-Resolution Using Deep Convolutional Networks.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Dong et al/Dong et al_2014_Image Super-Resolution Using Deep Convolutional Networks.pdf:application/pdf}
}

@incollection{liu_fast_2017,
	location = {Cham},
	title = {Fast and Accurate Image Super Resolution by Deep {CNN} with Skip Connection and Network in Network},
	volume = {10635},
	isbn = {978-3-319-70095-3 978-3-319-70096-0},
	url = {http://link.springer.com/10.1007/978-3-319-70096-0_23},
	abstract = {We propose a highly efficient and faster Single Image Super-Resolution ({SISR}) model with Deep Convolutional neural networks (Deep {CNN}). Deep {CNN} have recently shown that they have a significant reconstruction performance on single-image super-resolution. The current trend is using deeper {CNN} layers to improve performance. However, deep models demand larger computation resources and are not suitable for network edge devices like mobile, tablet and {IoT} devices. Our model achieves state-of-the-art reconstruction performance with at least 10 times lower calculation cost by Deep {CNN} with Residual Net, Skip Connection and Network in Network ({DCSCN}). A combination of Deep {CNNs} and Skip connection layers are used as a feature extractor for image features on both local and global areas. Parallelized 1x1 {CNNs}, like the one called Network in Network, are also used for image reconstruction. That structure reduces the dimensions of the previous layer’s output for faster computation with less information loss, and make it possible to process original images directly. Also we optimize the number of layers and filters of each {CNN} to significantly reduce the calculation cost. Thus, the proposed algorithm not only achieves stateof-the-art performance but also achieves faster and more efficient computation.},
	pages = {217--225},
	booktitle = {Neural Information Processing},
	publisher = {Springer International Publishing},
	author = {Yamanaka, Jin and Kuwashima, Shigesumi and Kurita, Takio},
	editor = {Liu, Derong and Xie, Shengli and Li, Yuanqing and Zhao, Dongbin and El-Alfy, El-Sayed M.},
	urldate = {2019-07-23},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-3-319-70096-0_23},
	file = {Yamanaka et al_2017_Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Yamanaka et al/Yamanaka et al_2017_Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and.pdf:application/pdf}
}

@article{wang_deep_2019,
	title = {Deep Learning for Image Super-resolution: A Survey},
	url = {http://arxiv.org/abs/1902.06068},
	shorttitle = {Deep Learning for Image Super-resolution},
	abstract = {Image Super-Resolution ({SR}) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. In this survey, we aim to give a survey on recent advances of image super-resolution techniques using deep learning approaches in a systematic way. In general, we can roughly group the existing studies of {SR} techniques into three major categories: supervised {SR}, unsupervised {SR}, and domain-speciﬁc {SR}. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.},
	journaltitle = {{arXiv}:1902.06068 [cs]},
	author = {Wang, Zhihao and Chen, Jian and Hoi, Steven C. H.},
	urldate = {2019-07-23},
	date = {2019-02-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.06068},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Wang et al_2019_Deep Learning for Image Super-resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Wang et al/Wang et al_2019_Deep Learning for Image Super-resolution.pdf:application/pdf}
}

@article{mahapatra_image_2019,
	title = {Image super-resolution using progressive generative adversarial networks for medical image analysis},
	volume = {71},
	issn = {08956111},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611118305871},
	doi = {10.1016/j.compmedimag.2018.10.005},
	pages = {30--39},
	journaltitle = {Computerized Medical Imaging and Graphics},
	shortjournal = {Computerized Medical Imaging and Graphics},
	author = {Mahapatra, Dwarikanath and Bozorgtabar, Behzad and Garnavi, Rahil},
	urldate = {2019-07-25},
	date = {2019-01},
	langid = {english},
	file = {Mahapatra et al_2019_Image super-resolution using progressive generative adversarial networks for medical image analysis.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Mahapatra et al/Mahapatra et al_2019_Image super-resolution using progressive generative adversarial networks for medical image analysis.pdf:application/pdf}
}

@software{majumdar_implementation_2019,
	title = {An implementation of {SRGAN} model in Keras. Contribute to titu1994/Super-Resolution-using-Generative-Adversarial-Networks development by creating an account on {GitHub}},
	url = {https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks},
	author = {Majumdar, Somshubra},
	urldate = {2019-07-25},
	date = {2019-06-21},
	note = {original-date: 2016-10-12T10:14:51Z}
}

@incollection{leal-taixe_esrgan:_2019,
	location = {Cham},
	title = {{ESRGAN}: Enhanced Super-Resolution Generative Adversarial Networks},
	volume = {11133},
	isbn = {978-3-030-11020-8 978-3-030-11021-5},
	url = {http://link.springer.com/10.1007/978-3-030-11021-5_5},
	shorttitle = {{ESRGAN}},
	abstract = {The Super-Resolution Generative Adversarial Network ({SRGAN}) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of {SRGAN} – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced {SRGAN} ({ESRGAN}). In particular, we introduce the Residual-in-Residual Dense Block ({RRDB}) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic {GAN} to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Beneﬁting from these improvements, the proposed {ESRGAN} achieves consistently better visual quality with more realistic and natural textures than {SRGAN} and won the ﬁrst place in the {PIRM}2018-{SR} Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/{ESRGAN}.},
	pages = {63--79},
	booktitle = {Computer Vision – {ECCV} 2018 Workshops},
	publisher = {Springer International Publishing},
	author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Loy, Chen Change},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	urldate = {2019-07-25},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-11021-5_5},
	file = {Wang et al_2019_ESRGAN.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Wang et al/Wang et al_2019_ESRGAN.pdf:application/pdf}
}

@article{anwar_deep_2019,
	title = {A Deep Journey into Super-resolution: A survey},
	url = {http://arxiv.org/abs/1904.07523},
	shorttitle = {A Deep Journey into Super-resolution},
	abstract = {Deep convolutional networks based super-resolution is a fast-growing ﬁeld with numerous practical applications. In this exposition, we extensively compare mora than 30 state-of-the-art super-resolution Convolutional Neural Networks ({CNNs}) over three classical and three recently introduced challenging datasets to benchmark single image super-resolution. We introduce a taxonomy for deep-learning based super-resolution networks that groups existing methods into nine categories including linear, residual, multi-branch, recursive, progressive, attention-based and adversarial designs. We also provide comparisons between the models in terms of network complexity, memory footprint, model input and output, learning details, the type of network losses and important architectural differences (e.g., depth, skip-connections, ﬁlters). The extensive evaluation performed, shows the consistent and rapid growth in the accuracy in the past few years along with a corresponding boost in model complexity and the availability of large-scale datasets. It is also observed that the pioneering methods identiﬁed as the benchmark have been signiﬁcantly outperformed by the current contenders. Despite the progress in recent years, we identify several shortcomings of existing techniques and provide future research directions towards the solution of these open problems.},
	journaltitle = {{arXiv}:1904.07523 [cs]},
	author = {Anwar, Saeed and Khan, Salman and Barnes, Nick},
	urldate = {2019-07-25},
	date = {2019-04-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.07523},
	file = {Anwar et al_2019_A Deep Journey into Super-resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Anwar et al/Anwar et al_2019_A Deep Journey into Super-resolution.pdf:application/pdf}
}

@article{gu_blind_2019,
	title = {Blind Super-Resolution With Iterative Kernel Correction},
	url = {http://arxiv.org/abs/1904.03377},
	abstract = {Deep learning based methods have dominated superresolution ({SR}) ﬁeld due to their remarkable performance in terms of effectiveness and efﬁciency. Most of these methods assume that the blur kernel during downsampling is predeﬁned/known (e.g., bicubic). However, the blur kernels involved in real applications are complicated and unknown, resulting in severe performance drop for the advanced {SR} methods. In this paper, we propose an Iterative Kernel Correction ({IKC}) method for blur kernel estimation in blind {SR} problem, where the blur kernels are unknown. We draw the observation that kernel mismatch could bring regular artifacts (either over-sharpening or over-smoothing), which can be applied to correct inaccurate blur kernels. Thus we introduce an iterative correction scheme – {IKC} that achieves better results than direct kernel estimation. We further propose an effective {SR} network architecture using spatial feature transform ({SFT}) layers to handle multiple blur kernels, named {SFTMD}. Extensive experiments on synthetic and real-world images show that the proposed {IKC} method with {SFTMD} can provide visually favorable {SR} results and the state-of-the-art performance in blind {SR} problem.},
	journaltitle = {{arXiv}:1904.03377 [cs]},
	author = {Gu, Jinjin and Lu, Hannan and Zuo, Wangmeng and Dong, Chao},
	urldate = {2019-07-25},
	date = {2019-04-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.03377},
	file = {Gu et al_2019_Blind Super-Resolution With Iterative Kernel Correction.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Gu et al/Gu et al_2019_Blind Super-Resolution With Iterative Kernel Correction.pdf:application/pdf}
}

@article{yuan_unsupervised_2018,
	title = {Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1809.00437},
	abstract = {We consider the single image super-resolution problem in a more general case that the low-/high-resolution pairs and the down-sampling process are unavailable. Different from traditional super-resolution formulation, the lowresolution input is further degraded by noises and blurring. This complicated setting makes supervised learning and accurate kernel estimation impossible. To solve this problem, we resort to unsupervised learning without paired data, inspired by the recent successful image-toimage translation applications. With generative adversarial networks ({GAN}) as the basic component, we propose a Cycle-in-Cycle network structure to tackle the problem within three steps. First, the noisy and blurry input is mapped to a noise-free low-resolution space. Then the intermediate image is up-sampled with a pre-trained deep model. Finally, we ﬁne-tune the two modules in an end-toend manner to get the high-resolution output. Experiments on {NTIRE}2018 datasets demonstrate that the proposed unsupervised method achieves comparable results as the stateof-the-art supervised models.},
	journaltitle = {{arXiv}:1809.00437 [cs]},
	author = {Yuan, Yuan and Liu, Siyuan and Zhang, Jiawei and Zhang, Yongbing and Dong, Chao and Lin, Liang},
	urldate = {2019-07-25},
	date = {2018-09-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1809.00437},
	file = {Yuan et al_2018_Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Yuan et al/Yuan et al_2018_Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks.pdf:application/pdf}
}

@software{garcia_image_2019,
	title = {Image super-resolution through deep learning. Contribute to david-gpu/srez development by creating an account on {GitHub}},
	rights = {{MIT}},
	url = {https://github.com/david-gpu/srez},
	author = {Garcia, David},
	urldate = {2019-07-25},
	date = {2019-07-25},
	note = {original-date: 2016-08-26T22:22:09Z}
}

@software{xintao_eccv18_2019,
	title = {{ECCV}18 Workshops: Enhanced {SRGAN}. Champion {PIRM} Challenge on Perceptual Super-Resolution (Third Region)  - xinntao/{ESRGAN}},
	rights = {Apache-2.0},
	url = {https://github.com/xinntao/ESRGAN},
	shorttitle = {{ECCV}18 Workshops},
	author = {Xintao},
	urldate = {2019-07-26},
	date = {2019-07-26},
	note = {original-date: 2018-08-31T08:18:41Z}
}

@article{sajjadi_enhancenet:_2016,
	title = {{EnhanceNet}: Single Image Super-Resolution Through Automated Texture Synthesis},
	url = {http://arxiv.org/abs/1612.07919},
	shorttitle = {{EnhanceNet}},
	abstract = {Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio ({PSNR}) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack highfrequency textures and do not look natural despite yielding high {PSNR} values.},
	journaltitle = {{arXiv}:1612.07919 [cs, stat]},
	author = {Sajjadi, Mehdi S. M. and Schölkopf, Bernhard and Hirsch, Michael},
	urldate = {2019-07-26},
	date = {2016-12-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1612.07919},
	file = {Sajjadi et al_2016_EnhanceNet.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Sajjadi et al/Sajjadi et al_2016_EnhanceNet.pdf:application/pdf}
}

@online{kanska_using_2017,
	title = {Using deep learning for Single Image Super Resolution},
	url = {https://deepsense.ai/using-deep-learning-for-single-image-super-resolution/},
	abstract = {We apply three different deep learning models to reproduce state-of-the-art results in single image super resolution.},
	titleaddon = {deepsense.ai},
	author = {Kańska, Katarzyna},
	urldate = {2019-07-28},
	date = {2017-10-23},
	langid = {american}
}

@article{wang_recovering_2018,
	title = {Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform},
	url = {http://arxiv.org/abs/1804.02815},
	abstract = {Despite that convolutional neural networks ({CNN}) have recently demonstrated high-quality reconstruction for single-image super-resolution ({SR}), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform ({SFT}) layer that generates afﬁne transformation parameters for spatial-wise feature modulation. {SFT} layers can be trained end-to-end together with the {SR} network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our ﬁnal results show that an {SR} network equipped with {SFT} can generate more realistic and visually pleasing textures in comparison to state-of-the-art {SRGAN} [27] and {EnhanceNet} [38].},
	journaltitle = {{arXiv}:1804.02815 [cs]},
	author = {Wang, Xintao and Yu, Ke and Dong, Chao and Loy, Chen Change},
	urldate = {2019-07-28},
	date = {2018-04-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.02815},
	file = {Wang et al_2018_Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Wang et al/Wang et al_2018_Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform.pdf:application/pdf}
}

@software{noauthor_idealo/image-super-resolution_2019,
	title = {idealo/image-super-resolution},
	rights = {Apache-2.0},
	url = {https://github.com/idealo/image-super-resolution},
	abstract = {Super-scale your images and run experiments with Residual Dense and Adversarial Networks.},
	publisher = {idealo},
	urldate = {2019-09-04},
	date = {2019-09-04},
	note = {original-date: 2018-11-26T13:41:13Z}
}

@article{yue_image_2016,
	title = {Image super-resolution: The techniques, applications, and future},
	volume = {128},
	issn = {01651684},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168416300536},
	doi = {10.1016/j.sigpro.2016.05.002},
	shorttitle = {Image super-resolution},
	abstract = {Super-resolution ({SR}) technique reconstructs a higher-resolution image or sequence from the observed {LR} images. As {SR} has been developed for more than three decades, both multi-frame and single-frame {SR} have signiﬁcant applications in our daily life. This paper aims to provide a review of {SR} from the perspective of techniques and applications, and especially the main contributions in recent years. Regularized {SR} methods are most commonly employed in the last decade. Technical details are discussed in this article, including reconstruction models, parameter selection methods, optimization algorithms and acceleration strategies. Moreover, an exhaustive summary of the current applications using {SR} techniques has been presented. Lastly, the article discusses the current obstacles for future research.},
	pages = {389--408},
	journaltitle = {Signal Processing},
	shortjournal = {Signal Processing},
	author = {Yue, Linwei and Shen, Huanfeng and Li, Jie and Yuan, Qiangqiang and Zhang, Hongyan and Zhang, Liangpei},
	urldate = {2019-09-23},
	date = {2016-11},
	langid = {english},
	file = {Yue et al. - 2016 - Image super-resolution The techniques, applicatio.pdf:/Users/mauulik/Zotero/storage/57HZVNGF/Yue et al. - 2016 - Image super-resolution The techniques, applicatio.pdf:application/pdf}
}

@article{rajan_generalized_2001,
	title = {Generalized interpolation and its application in super-resolution imaging},
	volume = {19},
	issn = {02628856},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885601000555},
	doi = {10.1016/S0262-8856(01)00055-5},
	abstract = {In this paper, we present a generalized interpolation scheme for image expansion and generation of super-resolution images. This is done by decomposing the image into appropriate subspaces, carrying out interpolation in individual subspaces and subsequently transforming the interpolated values back to the image domain. Various optical and structural properties of the image, such as 3-D shape of an object, regional homogeneity, local variations in scene re¯ectivity, etc., can be better preserved during the interpolation process. The motivation for doing so has also been explained theoretically. The generalized interpolation scheme is also shown to be useful in perceptually based high-resolution representation of images where interpolation is done on individual groups as per the perceptual necessity. Further, this scheme is also applied to generation of high-resolution transparencies from low resolution transparencies. q 2001 Elsevier Science B.V. All rights reserved.},
	pages = {957--969},
	number = {13},
	journaltitle = {Image and Vision Computing},
	shortjournal = {Image and Vision Computing},
	author = {Rajan, Deepu and Chaudhuri, Subhasis},
	urldate = {2019-09-23},
	date = {2001-11},
	langid = {english},
	file = {Rajan and Chaudhuri - 2001 - Generalized interpolation and its application in s.pdf:/Users/mauulik/Zotero/storage/W2T2VCTV/Rajan and Chaudhuri - 2001 - Generalized interpolation and its application in s.pdf:application/pdf}
}

@article{padalkar_digital_2016,
	title = {Digital Heritage Reconstruction Using Super-resolution and Inpainting},
	volume = {8},
	issn = {2469-4215, 2469-4223},
	url = {http://www.morganclaypool.com/doi/10.2200/S00740ED1V01Y201611VCP026},
	doi = {10.2200/S00740ED1V01Y201611VCP026},
	pages = {1--168},
	number = {8},
	journaltitle = {Synthesis Lectures on Visual Computing},
	shortjournal = {Synthesis Lectures on Visual Computing},
	author = {Padalkar, Milind G. and Joshi, Manjunath V. and Khatri, Nilay L.},
	urldate = {2019-09-23},
	date = {2016-12-12},
	langid = {english},
	file = {Padalkar et al. - 2016 - Digital Heritage Reconstruction Using Super-resolu.pdf:/Users/mauulik/Zotero/storage/K6YLDCV8/Padalkar et al. - 2016 - Digital Heritage Reconstruction Using Super-resolu.pdf:application/pdf}
}

@inproceedings{ledig_photo-realistic_2017-1,
	location = {Honolulu, {HI}},
	title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099502/},
	doi = {10.1109/CVPR.2017.19},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the ﬁner texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the ﬁdelity expected at the higher resolution. In this paper, we present {SRGAN}, a generative adversarial network ({GAN}) for image superresolution ({SR}). To our knowledge, it is the ﬁrst framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score ({MOS}) test shows hugely signiﬁcant gains in perceptual quality using {SRGAN}. The {MOS} scores obtained with {SRGAN} are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {105--114},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	urldate = {2019-09-23},
	date = {2017-07},
	langid = {english},
	file = {Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf:/Users/mauulik/Zotero/storage/CE8D6PT6/Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf:application/pdf}
}

@inproceedings{tolpekin_fuzzy_2008,
	location = {Boston, {MA}, {USA}},
	title = {Fuzzy Super Resolution Mapping Based on Markov Random Fields},
	isbn = {978-1-4244-2807-6},
	url = {http://ieeexplore.ieee.org/document/4779134/},
	doi = {10.1109/IGARSS.2008.4779134},
	abstract = {Recent research has used Markov Random Fields ({MRF}) as a method for super-resolution mapping ({SRM}). This paper investigated the per-pixel uncertainty associated with {MRF} based {SRM}. This provided insight into the spatial distribution of uncertainty associated with {SRM}. Furthermore, the map of per-pixel uncertainty clearly shows the boundary between land-cover classes and this may provide an input for image segmentation. The insight provided by the per-pixel uncertainty together with the class boundaries will be valuable for development of the {MRF} approach to super-resolution mapping.},
	eventtitle = {{IGARSS} 2008 - 2008 {IEEE} International Geoscience and Remote Sensing Symposium},
	pages = {II--875--II--878},
	booktitle = {{IGARSS} 2008 - 2008 {IEEE} International Geoscience and Remote Sensing Symposium},
	publisher = {{IEEE}},
	author = {Tolpekin, V. A. and Hamm, N.A.S.},
	urldate = {2019-09-23},
	date = {2008},
	langid = {english},
	file = {Tolpekin and Hamm - 2008 - Fuzzy Super Resolution Mapping Based on Markov Ran.pdf:/Users/mauulik/Zotero/storage/5C32ZT5E/Tolpekin and Hamm - 2008 - Fuzzy Super Resolution Mapping Based on Markov Ran.pdf:application/pdf}
}

@inproceedings{rezayi_huber_2017,
	location = {Isfahan, Iran},
	title = {Huber Markov random field for joint super resolution},
	isbn = {978-1-5386-4405-8},
	url = {http://ieeexplore.ieee.org/document/8342375/},
	doi = {10.1109/IranianMVIP.2017.8342375},
	abstract = {Super Resolution ({SR}) technique takes a sequence of blurred noisy Low Resolution ({LR}) images of a scene and fuses them to produce a High Resolution ({HR}) image with better quality. The {LR} images should be aligned ﬁrst. The most important {SR} methods are iterative simultaneous methods in which image fusion and image alignment are performed, simultaneously. In these methods, the {SR} problem is converted to an optimization problem in terms of the {HR} image and motion parameters. There are two major groups of simultaneous methods, i.e. joint methods and Alternating Minimization ({AM}) methods. The main difference between these two groups is that in the joint methods the correlation between {HR} image and motion parameters is fully considered in each iteration but in the {AM} methods this is not the case. Because the {SR} problem is ill-posed, to obtain unique solution, prior term is required for {HR} image. The most famous image priors are Gaussian Markov random ﬁeld ({GMRF}), Total Variation ({TV}) and their mixture, i.e. Huber Markov random ﬁeld ({HMRF}) which inherits the advantages of both {GMRF} and {TV} priors. But this new prior is non-quadratic, so can only used in {AM} methods. In this paper, after proposing a modiﬁed form for Huber potential function, a quasi-quadratic shape for {HMRF} prior is developed. Then using this new shape of {HMRF} a joint {SR} method is proposed. The experimental results on the synthetic and real sequences of {LR} images show the superiority of the proposed {SR} method.},
	eventtitle = {2017 10th Iranian Conference on Machine Vision and Image Processing ({MVIP})},
	pages = {93--98},
	booktitle = {2017 10th Iranian Conference on Machine Vision and Image Processing ({MVIP})},
	publisher = {{IEEE}},
	author = {Rezayi, Hossein and Seyedin, Seyed Alireza},
	urldate = {2019-09-23},
	date = {2017-11},
	langid = {english},
	file = {Rezayi and Seyedin - 2017 - Huber Markov random field for joint super resoluti.pdf:/Users/mauulik/Zotero/storage/ZQJ38U93/Rezayi and Seyedin - 2017 - Huber Markov random field for joint super resoluti.pdf:application/pdf}
}

@article{zhang_map-mrf-based_2018,
	title = {{MAP}-{MRF}-Based Super-Resolution Reconstruction Approach for Coded Aperture Compressive Temporal Imaging},
	volume = {8},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/8/3/338},
	doi = {10.3390/app8030338},
	abstract = {Coded Aperture Compressive Temporal Imaging ({CACTI}) can afford low-cost temporal super-resolution ({SR}), but limits are imposed by noise and compression ratio on reconstruction quality. To utilize inter-frame redundant information from multiple observations and sparsity in multi-transform domains, a robust reconstruction approach based on maximum a posteriori probability and Markov random ﬁeld ({MAP}-{MRF}) model for {CACTI} is proposed. The proposed approach adopts a weighted 3D neighbor system ({WNS}) and the coordinate descent method to perform joint estimation of model parameters, to achieve the robust super-resolution reconstruction. The proposed multi-reconstruction algorithm considers both total variation ({TV}) and 2,1 norm in wavelet domain to address the minimization problem for compressive sensing, and solves it using an accelerated generalized alternating projection algorithm. The weighting coefﬁcient for different regularizations and frames is resolved by the motion characteristics of pixels. The proposed approach can provide high visual quality in the foreground and background of a scene simultaneously and enhance the ﬁdelity of the reconstruction results. Simulation results have veriﬁed the efﬁcacy of our new optimization framework and the proposed reconstruction approach.},
	pages = {338},
	number = {3},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Zhang, Tinghua and Gao, Kun},
	urldate = {2019-09-23},
	date = {2018-02-27},
	langid = {english},
	file = {Zhang and Gao - 2018 - MAP-MRF-Based Super-Resolution Reconstruction Appr.pdf:/Users/mauulik/Zotero/storage/87ULPVKA/Zhang and Gao - 2018 - MAP-MRF-Based Super-Resolution Reconstruction Appr.pdf:application/pdf}
}

@article{min_li_markov_2008,
	title = {Markov Random Field Model-Based Edge-Directed Image Interpolation},
	volume = {17},
	issn = {1057-7149, 1941-0042},
	url = {http://ieeexplore.ieee.org/document/4530743/},
	doi = {10.1109/TIP.2008.924289},
	abstract = {This paper presents an edge-directed image interpolation algorithm. In the proposed algorithm, the edge directions are implicitly estimated with a statistical-based approach. In opposite to explicit edge directions, the local edge directions are indicated by length-16 weighting vectors. Implicitly, the weighting vectors are used to formulate geometric regularity ({GR}) constraint (smoothness along edges and sharpness across edges) and the {GR} constraint is imposed on the interpolated image through the Markov random ﬁeld ({MRF}) model. Furthermore, under the maximum a posteriori-{MRF} framework, the desired interpolated image corresponds to the minimal energy state of a 2-D random ﬁeld given the low-resolution image. Simulated annealing methods are used to search for the minimal energy state from the state space. To lower the computational complexity of {MRF}, a single-pass implementation is designed, which performs nearly as well as the iterative optimization. Simulation results show that the proposed {MRF} model-based edge-directed interpolation method produces edges with strong geometric regularity. Compared to traditional methods and other edge-directed interpolation methods, the proposed method improves the subjective quality of the interpolated edges while maintaining a high {PSNR} level.},
	pages = {1121--1128},
	number = {7},
	journaltitle = {{IEEE} Transactions on Image Processing},
	shortjournal = {{IEEE} Trans. on Image Process.},
	author = {{Min Li} and Nguyen, T.Q.},
	urldate = {2019-09-23},
	date = {2008-07},
	langid = {english},
	file = {Min Li and Nguyen - 2008 - Markov Random Field Model-Based Edge-Directed Imag.pdf:/Users/mauulik/Zotero/storage/B7SJ447Z/Min Li and Nguyen - 2008 - Markov Random Field Model-Based Edge-Directed Imag.pdf:application/pdf}
}


@article{jang_depth_2016,
	title = {Depth map generation using a single image sensor with phase masks},
	volume = {24},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/abstract.cfm?URI=oe-24-12-12868},
	doi = {10.1364/OE.24.012868},
	abstract = {Conventional stereo matching systems generate a depth map using two or more digital imaging sensors. It is difﬁcult to use the small camera system because of their high costs and bulky sizes. In order to solve this problem, this paper presents a stereo matching system using a single image sensor with phase masks for the phase difference auto-focusing. A novel pattern of phase mask array is proposed to simultaneously acquire two pairs of stereo images. Furthermore, a noise-invariant depth map is generated from the raw format sensor output. The proposed method consists of four steps to compute the depth map: (i) acquisition of stereo images using the proposed mask array, (ii) variational segmentation using merging criteria to simplify the input image, (iii) disparity map generation using the hierarchical block matching for disparity measurement, and (iv) image matting to ﬁll holes to generate the dense depth map. The proposed system can be used in small digital cameras without additional lenses or sensors.},
	pages = {12868},
	number = {12},
	journaltitle = {Optics Express},
	shortjournal = {Opt. Express},
	author = {Jang, Jinbeum and Park, Sangwoo and Jo, Jieun and Paik, Joonki},
	urldate = {2019-07-22},
	date = {2016-06-13},
	langid = {english},
	file = {Jang et al_2016_Depth map generation using a single image sensor with phase masks.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Jang et al/Jang et al_2016_Depth map generation using a single image sensor with phase masks.pdf:application/pdf}
}

@article{freeman_example-based_2002,
	title = {Example-based super-resolution},
	volume = {22},
	issn = {02721716},
	url = {http://ieeexplore.ieee.org/document/988747/},
	doi = {10.1109/38.988747},
	pages = {56--65},
	number = {2},
	journaltitle = {{IEEE} Computer Graphics and Applications},
	shortjournal = {{IEEE} Comput. Grap. Appl.},
	author = {Freeman, W.T. and Jones, T.R. and Pasztor, E.C.},
	urldate = {2019-07-22},
	date = {2002-04},
	langid = {english},
	file = {Freeman et al_2002_Example-based super-resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Freeman et al/Freeman et al_2002_Example-based super-resolution.pdf:application/pdf}
}

@article{zhao_fast_2015,
	title = {Fast Single Image Super-Resolution},
	url = {http://arxiv.org/abs/1510.00143},
	abstract = {This paper addresses the problem of single image super-resolution ({SR}), which consists of recovering a high resolution image from its blurred, decimated and noisy version. The existing algorithms for single image {SR} use different strategies to handle the decimation and blurring operators. In addition to the traditional ﬁrst-order gradient methods, recent techniques investigate splitting-based methods dividing the {SR} problem into up-sampling and deconvolution steps that can be easily solved. Instead of following this splitting strategy, we propose to deal with the decimation and blurring operators simultaneously by taking advantage of their particular properties in the frequency domain, leading to a new fast {SR} approach. Speciﬁcally, an analytical solution can be obtained and implemented efﬁciently for the Gaussian prior or any other regularization that can be formulated into an 2-regularized quadratic model, i.e., an 2- 2 optimization problem. Furthermore, the ﬂexibility of the proposed {SR} scheme is shown through the use of various priors/regularizations, ranging from generic image priors to learning-based approaches. In the case of non-Gaussian priors, we show how the analytical solution derived from the Gaussian case can be embedded into traditional splitting frameworks, allowing the computation cost of existing algorithms to be decreased signiﬁcantly. Simulation results conducted on several images with different priors illustrate the effectiveness of our fast {SR} approach compared with the existing techniques.},
	journaltitle = {{arXiv}:1510.00143 [cs]},
	author = {Zhao, Ningning and Wei, Qi and Basarab, Adrian and Dobigeon, Nicolas and Kouame, Denis and Tourneret, Jean-Yves},
	urldate = {2019-07-22},
	date = {2015-10-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1510.00143},
	file = {Zhao et al_2015_Fast Single Image Super-Resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Zhao et al/Zhao et al_2015_Fast Single Image Super-Resolution.pdf:application/pdf}
}

@inproceedings{ledig_photo-realistic_2017,
	location = {Honolulu, {HI}},
	title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099502/},
	doi = {10.1109/CVPR.2017.19},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the ﬁner texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the ﬁdelity expected at the higher resolution. In this paper, we present {SRGAN}, a generative adversarial network ({GAN}) for image superresolution ({SR}). To our knowledge, it is the ﬁrst framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score ({MOS}) test shows hugely signiﬁcant gains in perceptual quality using {SRGAN}. The {MOS} scores obtained with {SRGAN} are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {105--114},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	urldate = {2019-07-22},
	date = {2017-07},
	langid = {english},
	file = {Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Ledig et al/Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:application/pdf}
}

@inproceedings{glasner_super-resolution_2009,
	location = {Kyoto},
	title = {Super-resolution from a single image},
	isbn = {978-1-4244-4420-5},
	url = {http://ieeexplore.ieee.org/document/5459271/},
	doi = {10.1109/ICCV.2009.5459271},
	abstract = {Methods for super-resolution can be broadly classiﬁed into two families of methods: (i) The classical multi-image super-resolution (combining images obtained at subpixel misalignments), and (ii) Example-Based super-resolution (learning correspondence between low and high resolution image patches from a database). In this paper we propose a uniﬁed framework for combining these two families of methods. We further show how this combined approach can be applied to obtain super resolution from as little as a single image (with no database or prior examples). Our approach is based on the observation that patches in a natural image tend to redundantly recur many times inside the image, both within the same scale, as well as across different scales. Recurrence of patches within the same image scale (at subpixel misalignments) gives rise to the classical super-resolution, whereas recurrence of patches across different scales of the same image gives rise to example-based super-resolution. Our approach attempts to recover at each pixel its best possible resolution increase based on its patch redundancy within and across scales.},
	eventtitle = {2009 {IEEE} 12th International Conference on Computer Vision ({ICCV})},
	pages = {349--356},
	booktitle = {2009 {IEEE} 12th International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Glasner, Daniel and Bagon, Shai and Irani, Michal},
	urldate = {2019-07-22},
	date = {2009-09},
	langid = {english},
	file = {Glasner et al_2009_Super-resolution from a single image.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Glasner et al/Glasner et al_2009_Super-resolution from a single image.pdf:application/pdf}
}

@article{al-falluji_single_nodate,
	title = {Single Image Super Resolution Algorithms: A Survey and Evaluation},
	volume = {6},
	abstract = {Image processing sub branch that specifically deals with the improvement, of images and videos, resolution without compromising the detail and visual effect but rather enhances the two, is known as Super Resolution. Multiple (multiple input images and one output image) or single (one input and one output) low resolution images are converted to high resolution. Single image super resolution algorithms are more practical since multiple images are not always available. The paper presents a survey of recent single image super resolution methods that are based on the use of external database to predict the values of missing pixels in high resolution image.},
	pages = {8},
	number = {9},
	author = {Al-falluji, Ruaa Adeeb Abdulmunem and Youssif, Aliaa Abdel-Halim and Guirguis, Shawkat K},
	langid = {english},
	file = {Al-falluji et al_Single Image Super Resolution Algorithms.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Al-falluji et al/Al-falluji et al_Single Image Super Resolution Algorithms.pdf:application/pdf}
}

@article{wang_fully_2018,
	title = {A Fully Progressive Approach to Single-Image Super-Resolution},
	url = {http://arxiv.org/abs/1804.02900},
	abstract = {Recent deep learning approaches to single image superresolution have achieved impressive results in terms of traditional error measures and perceptual quality. However, in each case it remains challenging to achieve high quality results for large upsampling factors. To this end, we propose a method ({ProSR}) that is progressive both in architecture and training: the network upsamples an image in intermediate steps, while the learning process is organized from easy to hard, as is done in curriculum learning. To obtain more photorealistic results, we design a generative adversarial network ({GAN}), named {ProGanSR}, that follows the same progressive multi-scale design principle. This not only allows to scale well to high upsampling factors (e.g., 8×) but constitutes a principled multi-scale approach that increases the reconstruction quality for all upsampling factors simultaneously. In particular {ProSR} ranks 2nd in terms of {SSIM} and 4th in terms of {PSNR} in the {NTIRE}2018 {SISR} challenge [34]. Compared to the top-ranking team, our model is marginally lower, but runs 5 times faster.},
	journaltitle = {{arXiv}:1804.02900 [cs]},
	author = {Wang, Yifan and Perazzi, Federico and {McWilliams}, Brian and Sorkine-Hornung, Alexander and Sorkine-Hornung, Olga and Schroers, Christopher},
	urldate = {2019-07-23},
	date = {2018-04-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.02900},
	file = {Wang et al_2018_A Fully Progressive Approach to Single-Image Super-Resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Wang et al/Wang et al_2018_A Fully Progressive Approach to Single-Image Super-Resolution.pdf:application/pdf}
}

@article{kim_accurate_2015,
	title = {Accurate Image Super-Resolution Using Very Deep Convolutional Networks},
	url = {http://arxiv.org/abs/1511.04587},
	abstract = {We present a highly accurate single-image superresolution ({SR}) method. Our method uses a very deep convolutional network inspired by {VGG}-net used for {ImageNet} classiﬁcation [19]. We ﬁnd increasing our network depth shows a signiﬁcant improvement in accuracy. Our ﬁnal model uses 20 weight layers. By cascading small ﬁlters many times in a deep network structure, contextual information over large image regions is exploited in an efﬁcient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than {SRCNN} [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.},
	journaltitle = {{arXiv}:1511.04587 [cs]},
	author = {Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu},
	urldate = {2019-07-23},
	date = {2015-11-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.04587},
	file = {Kim et al_2015_Accurate Image Super-Resolution Using Very Deep Convolutional Networks.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Kim et al/Kim et al_2015_Accurate Image Super-Resolution Using Very Deep Convolutional Networks.pdf:application/pdf}
}

@article{shi_real-time_2016,
	title = {Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network},
	url = {http://arxiv.org/abs/1609.05158},
	abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution ({LR}) input image is upscaled to the high resolution ({HR}) space using a single ﬁlter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution ({SR}) operation is performed in {HR} space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the ﬁrst convolutional neural network ({CNN}) capable of real-time {SR} of 1080p videos on a single K2 {GPU}. To achieve this, we propose a novel {CNN} architecture where the feature maps are extracted in the {LR} space. In addition, we introduce an efﬁcient sub-pixel convolution layer which learns an array of upscaling ﬁlters to upscale the ﬁnal {LR} feature maps into the {HR} output. By doing so, we effectively replace the handcrafted bicubic ﬁlter in the {SR} pipeline with more complex upscaling ﬁlters speciﬁcally trained for each feature map, whilst also reducing the computational complexity of the overall {SR} operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs signiﬁcantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous {CNN}-based methods.},
	journaltitle = {{arXiv}:1609.05158 [cs, stat]},
	author = {Shi, Wenzhe and Caballero, Jose and Huszár, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	urldate = {2019-09-04},
	date = {2016-09-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1609.05158},
	file = {Shi et al. - 2016 - Real-Time Single Image and Video Super-Resolution .pdf:/Users/mauulik/Zotero/storage/BL8TTJ7T/Shi et al. - 2016 - Real-Time Single Image and Video Super-Resolution .pdf:application/pdf}
}


@article{kawulok_deep_2019,
	title = {Deep Learning for Multiple-Image Super-Resolution},
	url = {http://arxiv.org/abs/1903.00440},
	abstract = {Super-resolution reconstruction ({SRR}) is a process aimed at enhancing spatial resolution of images, either from a single observation, based on the learned relation between low and high resolution, or from multiple images presenting the same scene. {SRR} is particularly valuable, if it is infeasible to acquire images at desired resolution, but many images of the same scene are available at lower resolution—this is inherent to a variety of remote sensing scenarios. Recently, we have witnessed substantial improvement in single-image {SRR} attributed to the use of deep neural networks for learning the relation between low and high resolution. Importantly, deep learning has not been exploited for multiple-image {SRR}, which beneﬁts from information fusion and in general allows for achieving higher reconstruction accuracy. In this letter, we introduce a new method which combines the advantages of multiple-image fusion with learning the low-tohigh resolution mapping using deep networks. The reported experimental results indicate that our algorithm outperforms the state-of-the-art {SRR} methods, including these that operate from a single image, as well as those that perform multiple-image fusion.},
	journaltitle = {{arXiv}:1903.00440 [cs]},
	author = {Kawulok, Michal and Benecki, Pawel and Piechaczek, Szymon and Hrynczenko, Krzysztof and Kostrzewa, Daniel and Nalepa, Jakub},
	urldate = {2019-07-22},
	date = {2019-03-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.00440},
	file = {Kawulok et al_2019_Deep Learning for Multiple-Image Super-Resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Kawulok et al/Kawulok et al_2019_Deep Learning for Multiple-Image Super-Resolution.pdf:application/pdf}
}

@article{farsiu_fast_2004,
	title = {Fast and Robust Multiframe Super Resolution},
	volume = {13},
	issn = {1057-7149},
	url = {http://ieeexplore.ieee.org/document/1331445/},
	doi = {10.1109/TIP.2004.834669},
	abstract = {Super-resolution reconstruction produces one or a set of high-resolution images from a set of low-resolution images. In the last two decades, a variety of super-resolution methods have been proposed. These methods are usually very sensitive to their assumed model of data and noise, which limits their utility. This paper reviews some of these methods and addresses their shortcomings. We propose an alternate approach using 1 norm minimization and robust regularization based on a bilateral prior to deal with different data and noise models. This computationally inexpensive method is robust to errors in motion and blur estimation and results in images with sharp edges. Simulation results conﬁrm the effectiveness of our method and demonstrate its superiority to other super-resolution methods.},
	pages = {1327--1344},
	number = {10},
	journaltitle = {{IEEE} Transactions on Image Processing},
	shortjournal = {{IEEE} Trans. on Image Process.},
	author = {Farsiu, S. and Robinson, M.D. and Elad, M. and Milanfar, P.},
	urldate = {2019-07-22},
	date = {2004-10},
	langid = {english},
	file = {Farsiu et al_2004_Fast and Robust Multiframe Super Resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Farsiu et al/Farsiu et al_2004_Fast and Robust Multiframe Super Resolution.pdf:application/pdf}
}

@article{zhang_image_2018,
	title = {Image Super-Resolution Using Very Deep Residual Channel Attention Networks},
	url = {http://arxiv.org/abs/1807.02758},
	abstract = {Convolutional neural network ({CNN}) depth is of crucial importance for image super-resolution ({SR}). However, we observe that deeper networks for image {SR} are more diﬃcult to train. The lowresolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of {CNNs}. To solve these problems, we propose the very deep residual channel attention networks ({RCAN}). Speciﬁcally, we propose a residual in residual ({RIR}) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, {RIR} allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our {RCAN} achieves better accuracy and visual improvements against state-of-the-art methods.},
	journaltitle = {{arXiv}:1807.02758 [cs]},
	author = {Zhang, Yulun and Li, Kunpeng and Li, Kai and Wang, Lichen and Zhong, Bineng and Fu, Yun},
	urldate = {2019-07-22},
	date = {2018-07-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.02758},
	file = {Zhang et al_2018_Image Super-Resolution Using Very Deep Residual Channel Attention Networks.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Zhang et al/Zhang et al_2018_Image Super-Resolution Using Very Deep Residual Channel Attention Networks.pdf:application/pdf}
}

@article{del_gallego_multiple-image_2017,
	title = {Multiple-image super-resolution on mobile devices: an image warping approach},
	volume = {2017},
	issn = {1687-5281},
	url = {https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-016-0156-z},
	doi = {10.1186/s13640-016-0156-z},
	shorttitle = {Multiple-image super-resolution on mobile devices},
	abstract = {This paper discusses a super-resolution ({SR}) system implemented on a mobile device. We utilized an Android device’s camera to take successive shots and applied a classical multiple-image super-resolution ({SR}) technique that utilized a set of low-resolution ({LR}) images. Images taken from the mobile device are subjected to our proposed filtering scheme wherein images that have noticeable presence of blur are discarded to avoid outliers from affecting the produced high-resolution ({HR}) image. The remaining subset of images are subjected to non-local means denoising, then feature-matched against the first reference {LR} image. Successive images are then aligned with respect to the first image via affine and perspective warping transformations. The {LR} images are then upsampled using bicubic interpolation. An L2-norm minimization approach, which is essentially taking the pixel-wise mean of the aligned images, is performed to produce the final {HR} image. Our study shows that our proposed method performs better than the bicubic interpolation, which makes its implementation in a mobile device quite feasible. We have also proven in our experiments that there are substantial differences from images captured using burst mode that can be utilized by an {SR} algorithm to create an {HR} image.},
	pages = {8},
	number = {1},
	journaltitle = {{EURASIP} Journal on Image and Video Processing},
	shortjournal = {J Image Video Proc.},
	author = {Del Gallego, Neil Patrick and Ilao, Joel},
	urldate = {2019-07-22},
	date = {2017-12},
	langid = {english},
	file = {Del Gallego_Ilao_2017_Multiple-image super-resolution on mobile devices.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Del Gallego_Ilao/Del Gallego_Ilao_2017_Multiple-image super-resolution on mobile devices.pdf:application/pdf}
}

@incollection{leibe_perceptual_2016,
	location = {Cham},
	title = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
	volume = {9906},
	isbn = {978-3-319-46474-9 978-3-319-46475-6},
	url = {http://link.springer.com/10.1007/978-3-319-46475-6_43},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by deﬁning and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the beneﬁts of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	pages = {694--711},
	booktitle = {Computer Vision – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	urldate = {2019-07-22},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-46475-6_43},
	file = {Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Johnson et al/Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:application/pdf}
}

@incollection{ferrari_srfeat:_2018,
	location = {Cham},
	title = {{SRFeat}: Single Image Super-Resolution with Feature Discrimination},
	volume = {11220},
	isbn = {978-3-030-01269-4 978-3-030-01270-0},
	url = {http://link.springer.com/10.1007/978-3-030-01270-0_27},
	shorttitle = {{SRFeat}},
	abstract = {Generative adversarial networks ({GANs}) have recently been adopted to single image super-resolution ({SISR}) and showed impressive results with realistically synthesized high-frequency textures. However, the results of such {GAN}-based approaches tend to include less meaningful high-frequency noise that is irrelevant to the input image. In this paper, we propose a novel {GAN}-based {SISR} method that overcomes the limitation and produces more realistic results by attaching an additional discriminator that works in the feature domain. Our additional discriminator encourages the generator to produce structural high-frequency features rather than noisy artifacts as it distinguishes synthetic and real images in terms of features. We also design a new generator that utilizes long-range skip connections so that information between distant layers can be transferred more eﬀectively. Experiments show that our method achieves the state-of-the-art performance in terms of both {PSNR} and perceptual quality compared to recent {GAN}-based methods.},
	pages = {455--471},
	booktitle = {Computer Vision – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Park, Seong-Jin and Son, Hyeongseok and Cho, Sunghyun and Hong, Ki-Sang and Lee, Seungyong},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	urldate = {2019-07-22},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-030-01270-0_27},
	file = {Park et al_2018_SRFeat.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Park et al/Park et al_2018_SRFeat.pdf:application/pdf}
}

@inproceedings{capel_super-resolution_2001,
	location = {Kauai, {HI}, {USA}},
	title = {Super-resolution from multiple views using learnt image models},
	volume = {2},
	isbn = {978-0-7695-1272-3},
	url = {http://ieeexplore.ieee.org/document/991022/},
	doi = {10.1109/CVPR.2001.991022},
	abstract = {The objective of this work is the super-resolution restoration of a set of images, and we investigate the use of learnt image models within a generative Bayesian framework.},
	eventtitle = {2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	pages = {II--627--II--634},
	booktitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	publisher = {{IEEE} Comput. Soc},
	author = {Capel, D. and Zisserman, A.},
	urldate = {2019-07-22},
	date = {2001},
	langid = {english},
	file = {Capel_Zisserman_2001_Super-resolution from multiple views using learnt image models.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Capel_Zisserman/Capel_Zisserman_2001_Super-resolution from multiple views using learnt image models.pdf:application/pdf}
}

@inproceedings{borman_super-resolution_1999,
	location = {Notre Dame, {IN}, {USA}},
	title = {Super-resolution from image sequences-a review},
	isbn = {978-0-8186-8914-7},
	url = {http://ieeexplore.ieee.org/document/759509/},
	doi = {10.1109/MWSCAS.1998.759509},
	abstract = {Growing interest in super-resolution ({SR}) restoration of video sequences and the closely related problem of construction of {SR} still images from image sequences has led to the emergence of several competing methodologies. We review the state of the art of {SR} techniques using a taxonomy of existing techniques. We critique these methods and identify areas which promise performance improvements.},
	eventtitle = {1998 Midwest Symposium on Circuits and Systems},
	pages = {374--378},
	booktitle = {1998 Midwest Symposium on Circuits and Systems (Cat. No. 98CB36268)},
	publisher = {{IEEE} Comput. Soc},
	author = {Borman, S. and Stevenson, R.L.},
	urldate = {2019-07-22},
	date = {1999},
	langid = {english},
	file = {Borman_Stevenson_1999_Super-resolution from image sequences-a review.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Borman_Stevenson/Borman_Stevenson_1999_Super-resolution from image sequences-a review.pdf:application/pdf}
}

@article{najafi_single_nodate,
	title = {Single and Multi-view Video Super-resolution},
	pages = {97},
	author = {Najafi, Seyedreza},
	langid = {english},
	file = {Najafi_Single and Multi-view Video Super-resolution.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Najafi/Najafi_Single and Multi-view Video Super-resolution.pdf:application/pdf}
}

@article{chu_fast_2019,
	title = {Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search},
	url = {http://arxiv.org/abs/1901.07261},
	abstract = {Deep convolution neural networks demonstrate impressive results in the super-resolution domain. A series of studies concentrate on improving peak signal noise ratio ({PSNR}) by using much deeper layers, which are not friendly to constrained resources. Pursuing a trade-off between the restoration capacity and the simplicity of models is still non-trivial. Recent contributions are struggling to manually maximize this balance, while our work achieves the same goal automatically with neural architecture search. Speciﬁcally, we handle superresolution with a multi-objective approach. We also propose an elastic search tactic at both micro and macro level, based on a hybrid controller that profits from evolutionary computation and reinforcement learning. Quantitative experiments help us to draw a conclusion that our generated models dominate most of the state-of-the-art methods with respect to the individual {FLOPS}.},
	journaltitle = {{arXiv}:1901.07261 [cs]},
	author = {Chu, Xiangxiang and Zhang, Bo and Ma, Hailong and Xu, Ruijun and Li, Jixiang and Li, Qingyuan},
	urldate = {2019-07-22},
	date = {2019-01-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1901.07261},
	file = {Chu et al_2019_Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Chu et al/Chu et al_2019_Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search.pdf:application/pdf}
}

@inproceedings{garcia_super-resolution_2010,
	location = {Hong Kong, Hong Kong},
	title = {Super-resolution for multiview images using depth information},
	isbn = {978-1-4244-7992-4},
	url = {http://ieeexplore.ieee.org/document/5651388/},
	doi = {10.1109/ICIP.2010.5651388},
	abstract = {Mixed resolution formats have been employed in video encoding complexity reduction as well as data compression of stereoscopic video. High resolution frames within such formats may also be used as a means of enhancing lower resolution images. In this paper we present a super-resolution method for use in a mixed resolution, multiview video plus depth setup. High resolution views are initially projected onto the view point of low resolution images with the aid of available depth maps. The method introduces the use of transform-domain techniques for up-sampling the low resolution images and for appending high frequency content from projected views. The {DCT} is used for the necessary frequency decompositions. Results show gains over previous work for several test sequences and afﬁrm the aptitude of transform-domain approaches for super-resolution within mixed resolution formats.},
	eventtitle = {2010 17th {IEEE} International Conference on Image Processing ({ICIP} 2010)},
	pages = {1793--1796},
	booktitle = {2010 {IEEE} International Conference on Image Processing},
	publisher = {{IEEE}},
	author = {Garcia, Diogo C. and Dorea, Camilo and de Queiroz, Ricardo L.},
	urldate = {2019-07-22},
	date = {2010-09},
	langid = {english},
	file = {Garcia et al_2010_Super-resolution for multiview images using depth information.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Garcia et al/Garcia et al_2010_Super-resolution for multiview images using depth information.pdf:application/pdf}
}

@inproceedings{wu_multiple-image_2017,
	location = {Montreal, {QC}},
	title = {Multiple-image super resolution using both reconstruction optimization and deep neural network},
	isbn = {978-1-5090-5990-4},
	url = {http://ieeexplore.ieee.org/document/8309146/},
	doi = {10.1109/GlobalSIP.2017.8309146},
	abstract = {We present an efﬁcient multi-image super resolution ({MISR}) method. Our solution consists of a L1-norm optimized reconstruction scheme for super resolution ({SR}), and a three-layer convolutional network for artifacts removal, in a concatenated fashion. Such a two-stage method achieves excellent performance, which outperforms the existing state-of-the-art {SR} methods in both subjective and objective measurements (e.g., 5 to 7 {dB} improvements on popular image database using {PSNR} metric).},
	eventtitle = {2017 {IEEE} Global Conference on Signal and Information Processing ({GlobalSIP})},
	pages = {1175--1179},
	booktitle = {2017 {IEEE} Global Conference on Signal and Information Processing ({GlobalSIP})},
	publisher = {{IEEE}},
	author = {Wu, Jie and Yue, Tao and Shen, Qiu and Cao, Xun and Ma, Zhan},
	urldate = {2019-07-22},
	date = {2017-11},
	langid = {english},
	file = {Wu et al_2017_Multiple-image super resolution using both reconstruction optimization and deep neural network.pdf:/Users/mauulik/Desktop/Research/zotero-library/zotero-library/AUTH/Wu et al/Wu et al_2017_Multiple-image super resolution using both reconstruction optimization and deep neural network.pdf:application/pdf}
}

@article{baltimore_image_nodate,
	title = {Image Super-Resolution},
	pages = {46},
	author = {Baltimore, Robert},
	langid = {english},
	file = {Baltimore - Image Super-Resolution.pdf:/Users/mauulik/Zotero/storage/R2UDSLFQ/Baltimore - Image Super-Resolution.pdf:application/pdf}
}

@software{izvorski_aizvorski/video-quality_2019,
	title = {aizvorski/video-quality},
	rights = {{GPL}-2.0},
	url = {https://github.com/aizvorski/video-quality},
	abstract = {Video quality metrics, reference implementation in python: {VIF}, {SSIM}, {PSNR}, ...},
	author = {Izvorski, Alex},
	urldate = {2019-09-04},
	date = {2019-09-03},
	note = {original-date: 2014-03-06T00:14:17Z}
}

